[
  {
    "question": "What is Retrieval-Augmented Generation?",
    "answer": "Retrieval-Augmented Generation (RAG) is an AI framework that enhances large language models by retrieving relevant information from external knowledge sources before generating responses. This approach helps improve accuracy, reduce hallucinations, and provide up-to-date information.",
    "relevant_docs": [
      {
        "content": "Retrieval-Augmented Generation (RAG) combines the strengths of retrieval-based and generation-based approaches in natural language processing. In RAG, an LLM first retrieves relevant information from a knowledge base and then uses this information to generate accurate and contextually relevant responses.",
        "metadata": {
          "source": "rag-overview",
          "start_index": 100
        }
      },
      {
        "content": "RAG addresses several limitations of traditional LLMs, such as hallucinations, outdated knowledge, and lack of domain-specific expertise. By incorporating external knowledge, RAG systems can provide more accurate, up-to-date, and verifiable responses.",
        "metadata": {
          "source": "rag-benefits",
          "start_index": 350
        }
      }
    ]
  },
  {
    "question": "What are the main components of a RAG system?",
    "answer": "A RAG system typically consists of three main components: (1) a retriever that finds relevant information from an external knowledge base, (2) an indexing system that organizes and stores the knowledge, and (3) a generator (LLM) that synthesizes the retrieved information into a coherent response.",
    "relevant_docs": [
      {
        "content": "RAG architectures generally comprise three key components. First, the retriever component searches for relevant information from an external knowledge source based on the input query. Second, the indexing system organizes and stores the knowledge in a format optimized for retrieval. Third, the generator (typically an LLM) takes both the original query and the retrieved information to produce a coherent and accurate response.",
        "metadata": {
          "source": "rag-architecture",
          "start_index": 780
        }
      }
    ]
  },
  {
    "question": "How does RAG help reduce hallucinations in LLMs?",
    "answer": "RAG reduces hallucinations by grounding the LLM's responses in retrieved factual information. When the model has access to relevant, accurate information before generating a response, it's less likely to fabricate facts. The retrieved context acts as a knowledge constraint that guides the generation process toward factual correctness.",
    "relevant_docs": [
      {
        "content": "One of the key benefits of RAG is its ability to reduce hallucinations in LLM outputs. Hallucinations occur when an LLM confidently generates false or misleading information. By retrieving relevant documents before generation, RAG provides the model with factual information, effectively grounding its responses in reality rather than relying solely on parametric knowledge.",
        "metadata": {
          "source": "rag-hallucination-reduction",
          "start_index": 1200
        }
      },
      {
        "content": "The retrieved context in RAG serves as a knowledge constraint during the generation phase. This helps guide the LLM to produce responses that align with the facts presented in the retrieved documents, significantly reducing the likelihood of fabricated information in the output.",
        "metadata": {
          "source": "rag-generation-constraints",
          "start_index": 1450
        }
      }
    ]
  },
  {
    "question": "What are the common evaluation metrics for RAG systems?",
    "answer": "Common evaluation metrics for RAG systems include: (1) Retrieval metrics like precision, recall, and MRR that assess the quality of retrieved information; (2) Generation metrics such as factual accuracy, relevance, and groundedness that evaluate response quality; and (3) System metrics including latency and cost that measure operational performance.",
    "relevant_docs": [
      {
        "content": "Evaluating RAG systems requires a multifaceted approach. Retrieval quality is typically measured using metrics like precision@k, recall@k, Mean Reciprocal Rank (MRR), and NDCG. These metrics assess how effectively the system retrieves relevant information from the knowledge base.",
        "metadata": {
          "source": "rag-evaluation-retrieval",
          "start_index": 2100
        }
      },
      {
        "content": "The generation component of RAG is evaluated using metrics like factual accuracy, relevance to the query, groundedness in the retrieved context, coherence, and conciseness. These metrics help determine if the generated response correctly uses the retrieved information and addresses the user's question effectively.",
        "metadata": {
          "source": "rag-evaluation-generation",
          "start_index": 2300
        }
      },
      {
        "content": "System-level metrics for RAG include latency (response time), throughput (queries processed per unit time), and cost (computational and financial resources required). These operational metrics are crucial for assessing the practical deployability of RAG systems.",
        "metadata": {
          "source": "rag-evaluation-system",
          "start_index": 2600
        }
      }
    ]
  },
  {
    "question": "What's the difference between embedding-based and sparse retrieval in RAG?",
    "answer": "Embedding-based retrieval in RAG uses dense vector representations to capture semantic meaning, enabling the discovery of conceptually similar content even with different terminology. Sparse retrieval, like BM25, relies on exact keyword matching. Hybrid approaches combine both methods to leverage semantic understanding and keyword precision.",
    "relevant_docs": [
      {
        "content": "RAG systems employ different retrieval mechanisms. Embedding-based (dense) retrieval converts documents and queries into vector representations that capture semantic meaning. This allows for finding conceptually similar content even when the exact wording differs. Popular embedding models include OpenAI embeddings, BERT, and sentence transformers.",
        "metadata": {
          "source": "rag-retrieval-methods",
          "start_index": 3000
        }
      },
      {
        "content": "Sparse retrieval methods like BM25 focus on term frequency and exact keyword matching rather than semantic understanding. While less sophisticated than embedding-based approaches, sparse retrieval can be more precise when exact terminology matters and requires less computational resources.",
        "metadata": {
          "source": "rag-retrieval-methods",
          "start_index": 3250
        }
      },
      {
        "content": "Hybrid retrieval combines dense and sparse approaches to leverage the strengths of both. For example, a system might use BM25 to find keyword matches and embedding similarity to capture semantic relationships, then merge the results. This approach often yields better performance than either method alone.",
        "metadata": {
          "source": "rag-retrieval-methods",
          "start_index": 3450
        }
      }
    ]
  }
] 